---
id: motivation
title: Motivation
description: "Spoiler: we built it so you can focus on awesome scrapers"
---

Thanks to the tools like [Playwright](https://github.com/microsoft/playwright), [Puppeteer](https://github.com/puppeteer/puppeteer) or [Cheerio](https://www.npmjs.com/package/cheerio), it is easy to write Node.js code to extract data from web pages. But eventually things will get complicated. For example, when we try to:

-   Perform a deep crawl of an entire website using a persistent queue of URLs.
-   Run our scraping code on a list of 100k URLs in a CSV file, without losing any data when our code crashes.
-   Rotate proxies to hide our browser origin and keep user-like sessions.
-   Disable browser fingerprinting protections used by websites.

Python has [Scrapy](https://scrapy.org/) for these tasks, but there was no such library for **JavaScript, the language of
the web**. The use of JavaScript is natural, since the same language is used to write the scripts as well as the data extraction code running in a browser.

The goal of Crawlee is to fill this gap and provide a toolbox for generic web scraping, crawling and automation tasks in JavaScript and TypeScript, so that we could focus on writing code specific to the target website, rather than developing
commonalities by reinventing the wheel every time we need data from the web.
