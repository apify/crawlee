"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[90560],{72516:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>u,contentTitle:()=>c,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var a=t(85893),n=t(11151),s=t(93e3),i=t(14959);const p="import { PuppeteerCrawler } from 'crawlee';\n\nconst crawler = new PuppeteerCrawler({\n    async requestHandler({ request, page, enqueueLinks, log }) {\n        const title = await page.title();\n        log.info(`Title of ${request.url}: ${title}`);\n\n        await enqueueLinks({\n            globs: ['http?(s)://www.iana.org/**'],\n        });\n    },\n    maxRequestsPerCrawl: 10,\n});\n\nawait crawler.addRequests(['https://www.iana.org/']);\n\nawait crawler.run();\n",o={id:"puppeteer-recursive-crawl",title:"Puppeteer recursive crawl"},c=void 0,l={id:"examples/puppeteer-recursive-crawl",title:"Puppeteer recursive crawl",description:"Run the following example to perform a recursive crawl of a website using PuppeteerCrawler.",source:"@site/versioned_docs/version-3.4/examples/puppeteer_recursive_crawl.mdx",sourceDirName:"examples",slug:"/examples/puppeteer-recursive-crawl",permalink:"/docs/3.4/examples/puppeteer-recursive-crawl",draft:!1,unlisted:!1,tags:[],version:"3.4",lastUpdatedBy:"Vlad Frangu",lastUpdatedAt:1704359836,formattedLastUpdatedAt:"Jan 4, 2024",frontMatter:{id:"puppeteer-recursive-crawl",title:"Puppeteer recursive crawl"},sidebar:"docs",previous:{title:"Puppeteer crawler",permalink:"/docs/3.4/examples/puppeteer-crawler"},next:{title:"Skipping navigations for certain requests",permalink:"/docs/3.4/examples/skip-navigation"}},u={},d=[];function w(e){const r={admonition:"admonition",code:"code",p:"p",...(0,n.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(r.p,{children:["Run the following example to perform a recursive crawl of a website using ",(0,a.jsx)(i.Z,{to:"puppeteer-crawler/class/PuppeteerCrawler",children:(0,a.jsx)(r.code,{children:"PuppeteerCrawler"})}),"."]}),"\n",(0,a.jsx)(r.admonition,{type:"tip",children:(0,a.jsxs)(r.p,{children:["To run this example on the Apify Platform, select the ",(0,a.jsx)(r.code,{children:"apify/actor-node-puppeteer-chrome"})," image for your Dockerfile."]})}),"\n",(0,a.jsx)(s.default,{className:"language-js",children:p})]})}function m(e={}){const{wrapper:r}={...(0,n.a)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(w,{...e})}):w(e)}},14959:(e,r,t)=>{t.d(r,{Z:()=>c});t(67294);var a=t(39960),n=t(74477),s=t(52263),i=t(85893),p=t(50643).version.split("."),o=[p[0],p[1]].join(".");const c=function(e){var r=e.to,t=e.children,p=(0,n.E)();return(0,s.default)().siteConfig.presets[0][1].docs.disableVersioning||p.version===o?(0,i.jsx)(a.default,{to:"/api/"+r,children:t}):(0,i.jsx)(a.default,{to:"/api/"+("current"===p.version?"next":p.version)+"/"+r,children:t})}},11151:(e,r,t)=>{t.d(r,{Z:()=>p,a:()=>i});var a=t(67294);const n={},s=a.createContext(n);function i(e){const r=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function p(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),a.createElement(s.Provider,{value:r},e.children)}},50643:e=>{e.exports=JSON.parse('{"name":"crawlee","version":"3.7.1","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.7.1","@crawlee/browser":"3.7.1","@crawlee/browser-pool":"3.7.1","@crawlee/cheerio":"3.7.1","@crawlee/cli":"3.7.1","@crawlee/core":"3.7.1","@crawlee/http":"3.7.1","@crawlee/jsdom":"3.7.1","@crawlee/linkedom":"3.7.1","@crawlee/playwright":"3.7.1","@crawlee/puppeteer":"3.7.1","@crawlee/utils":"3.7.1","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);