"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[59616],{26350:(e,t,r)=>{r.r(t),r.d(t,{contentTitle:()=>o,default:()=>l,frontMatter:()=>n,toc:()=>c});var s=r(85893),a=r(11151),n={},o=void 0,c=[{value:"Example usage",id:"example-usage",level:2}];function i(e){var t=Object.assign({a:"a",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong"},(0,a.a)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.p,{children:"Provides a framework for the parallel crawling of web pages using plain HTTP requests. The URLs to crawl are fed either from a static list of URLs or from a dynamic queue of URLs enabling recursive crawling of websites."}),"\n",(0,s.jsxs)(t.p,{children:["It is very fast and efficient on data bandwidth. However, if the target website requires JavaScript to display the content, you might need to use ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/puppeteer-crawler/class/PuppeteerCrawler",children:"PuppeteerCrawler"})," or ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/playwright-crawler/class/PlaywrightCrawler",children:"PlaywrightCrawler"})," instead, because it loads the pages using full-featured headless Chrome browser. ",(0,s.jsx)(t.strong,{children:"This crawler downloads each URL using a plain HTTP request and doesn't do any HTML parsing."})]}),"\n",(0,s.jsxs)(t.p,{children:["The source URLs are represented using ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/Request",children:"Request"})," objects that are fed from ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/RequestList",children:"RequestList"})," or ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/RequestQueue",children:"RequestQueue"})," instances provided by the ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/http-crawler/interface/HttpCrawlerOptions#requestList",children:"HttpCrawlerOptions.requestList"})," or ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/http-crawler/interface/HttpCrawlerOptions#requestQueue",children:"HttpCrawlerOptions.requestQueue"})," constructor options, respectively."]}),"\n",(0,s.jsxs)(t.p,{children:["If both ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/http-crawler/interface/HttpCrawlerOptions#requestList",children:"HttpCrawlerOptions.requestList"})," and ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/http-crawler/interface/HttpCrawlerOptions#requestQueue",children:"HttpCrawlerOptions.requestQueue"})," are used, the instance first processes URLs from the ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/RequestList",children:"RequestList"})," and automatically enqueues all of them to ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/RequestQueue",children:"RequestQueue"})," before it starts their processing. This ensures that a single URL is not crawled multiple times."]}),"\n",(0,s.jsxs)(t.p,{children:["The crawler finishes when there are no more ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/Request",children:"Request"})," objects to crawl."]}),"\n",(0,s.jsxs)(t.p,{children:["We can use the ",(0,s.jsx)(t.code,{children:"preNavigationHooks"})," to adjust ",(0,s.jsx)(t.code,{children:"gotOptions"}),":"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-javascript",children:"preNavigationHooks: [\n    (crawlingContext, gotOptions) => {\n        // ...\n    },\n]\n"})}),"\n",(0,s.jsxs)(t.p,{children:["By default, ",(0,s.jsx)(t.code,{children:"HttpCrawler"})," only processes web pages with the ",(0,s.jsx)(t.code,{children:"text/html"})," and ",(0,s.jsx)(t.code,{children:"application/xhtml+xml"})," MIME content types (as reported by the ",(0,s.jsx)(t.code,{children:"Content-Type"})," HTTP header), and skips pages with other content types. If you want the crawler to process other content types, use the ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/http-crawler/interface/HttpCrawlerOptions#additionalMimeTypes",children:"HttpCrawlerOptions.additionalMimeTypes"})," constructor option. Beware that the parsing behavior differs for HTML, XML, JSON and other types of content. For more details, see ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/http-crawler/interface/HttpCrawlerOptions#requestHandler",children:"HttpCrawlerOptions.requestHandler"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["New requests are only dispatched when there is enough free CPU and memory available, using the functionality provided by the ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/AutoscaledPool",children:"AutoscaledPool"})," class. All ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/AutoscaledPool",children:"AutoscaledPool"})," configuration options can be passed to the ",(0,s.jsx)(t.code,{children:"autoscaledPoolOptions"})," parameter of the ",(0,s.jsx)(t.code,{children:"HttpCrawler"})," constructor. For user convenience, the ",(0,s.jsx)(t.code,{children:"minConcurrency"})," and ",(0,s.jsx)(t.code,{children:"maxConcurrency"})," ",(0,s.jsx)(t.a,{href:"https://crawlee.dev/api/core/class/AutoscaledPool",children:"AutoscaledPool"})," options are available directly in the ",(0,s.jsx)(t.code,{children:"HttpCrawler"})," constructor."]}),"\n",(0,s.jsx)(t.h2,{id:"example-usage",children:"Example usage"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-javascript",children:"import { HttpCrawler, Dataset } from '@crawlee/http';\n\nconst crawler = new HttpCrawler({\n    requestList,\n    async requestHandler({ request, response, body, contentType }) {\n        // Save the data to dataset.\n        await Dataset.pushData({\n            url: request.url,\n            html: body,\n        });\n    },\n});\n\nawait crawler.run([\n    'http://www.example.com/page-1',\n    'http://www.example.com/page-2',\n]);\n"})})]})}function l(e){void 0===e&&(e={});var t=Object.assign({},(0,a.a)(),e.components).wrapper;return t?(0,s.jsx)(t,Object.assign({},e,{children:(0,s.jsx)(i,Object.assign({},e))})):i(e)}},11151:(e,t,r)=>{r.d(t,{Z:()=>c,a:()=>o});var s=r(67294);const a={},n=s.createContext(a);function o(e){const t=s.useContext(n);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(n.Provider,{value:t},e.children)}}}]);