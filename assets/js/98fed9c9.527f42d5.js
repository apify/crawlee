"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[61767],{74979:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>p,toc:()=>u});var r=a(85893),s=a(11151),n=a(93e3),o=a(14959);const i="import { Dataset, launchPuppeteer } from 'crawlee';\n\n// Launch the web browser.\nconst browser = await launchPuppeteer();\n\n// Create and navigate new page\nconsole.log('Open target page');\nconst page = await browser.newPage();\nawait page.goto('https://github.com/search/advanced');\n\n// Fill form fields and select desired search options\nconsole.log('Fill in search form');\nawait page.type('#adv_code_search input.js-advanced-search-input', 'apify-js');\nawait page.type('#search_from', 'apify');\nawait page.type('#search_date', '>2015');\nawait page.select('select#search_language', 'JavaScript');\n\n// Submit the form and wait for full load of next page\nconsole.log('Submit search form');\nawait Promise.all([\n    page.waitForNavigation(),\n    page.click('#adv_code_search button[type=\"submit\"]'),\n]);\n\n// Obtain and print list of search results\nconst results = await page.$$eval('div.f4.text-normal a', (nodes) => nodes.map((node) => ({\n    url: node.href,\n    name: node.innerText,\n})));\n\nconsole.log('Results:', results);\n\n// Store data in default dataset\nawait Dataset.pushData(results);\n\n// Close browser\nawait browser.close();\n",l={id:"forms",title:"Forms"},c=void 0,p={id:"examples/forms",title:"Forms",description:"This example demonstrates how to use PuppeteerCrawler to",source:"@site/versioned_docs/version-3.2/examples/forms.mdx",sourceDirName:"examples",slug:"/examples/forms",permalink:"/docs/3.2/examples/forms",draft:!1,unlisted:!1,tags:[],version:"3.2",lastUpdatedBy:"Vlad Frangu",lastUpdatedAt:1704359836,formattedLastUpdatedAt:"Jan 4, 2024",frontMatter:{id:"forms",title:"Forms"},sidebar:"docs",previous:{title:"Export entire dataset to one file",permalink:"/docs/3.2/examples/export-entire-dataset"},next:{title:"HTTP crawler",permalink:"/docs/3.2/examples/http-crawler"}},d={},u=[];function m(e){const t={a:"a",admonition:"admonition",code:"code",p:"p",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(t.p,{children:["This example demonstrates how to use ",(0,r.jsx)(o.Z,{to:"puppeteer-crawler/class/PuppeteerCrawler",children:(0,r.jsx)(t.code,{children:"PuppeteerCrawler"})})," to\nautomatically fill and submit a search form to look up repositories on ",(0,r.jsx)(t.a,{href:"https://github.com",target:"_blank",rel:"noopener",children:"GitHub"})," using headless Chrome / Puppeteer.\nThe crawler first fills in the search term, repository owner, start date and language of the repository, then submits the form\nand prints out the results. Finally, the results are saved either on the Apify platform to the\ndefault ",(0,r.jsx)(o.Z,{to:"core/class/Dataset",children:(0,r.jsx)(t.code,{children:"dataset"})})," or on the local machine as JSON files in ",(0,r.jsx)(t.code,{children:"./storage/datasets/default"}),"."]}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["To run this example on the Apify Platform, select the ",(0,r.jsx)(t.code,{children:"apify/actor-node-puppeteer-chrome"})," image for your Dockerfile."]})}),"\n",(0,r.jsx)(n.default,{className:"language-js",children:i})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},14959:(e,t,a)=>{a.d(t,{Z:()=>c});a(67294);var r=a(39960),s=a(74477),n=a(52263),o=a(85893),i=a(50643).version.split("."),l=[i[0],i[1]].join(".");const c=function(e){var t=e.to,a=e.children,i=(0,s.E)();return(0,n.default)().siteConfig.presets[0][1].docs.disableVersioning||i.version===l?(0,o.jsx)(r.default,{to:"/api/"+t,children:a}):(0,o.jsx)(r.default,{to:"/api/"+("current"===i.version?"next":i.version)+"/"+t,children:a})}},11151:(e,t,a)=>{a.d(t,{Z:()=>i,a:()=>o});var r=a(67294);const s={},n=r.createContext(s);function o(e){const t=r.useContext(n);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(n.Provider,{value:t},e.children)}},50643:e=>{e.exports=JSON.parse('{"name":"crawlee","version":"3.7.1","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.7.1","@crawlee/browser":"3.7.1","@crawlee/browser-pool":"3.7.1","@crawlee/cheerio":"3.7.1","@crawlee/cli":"3.7.1","@crawlee/core":"3.7.1","@crawlee/http":"3.7.1","@crawlee/jsdom":"3.7.1","@crawlee/linkedom":"3.7.1","@crawlee/playwright":"3.7.1","@crawlee/puppeteer":"3.7.1","@crawlee/utils":"3.7.1","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);